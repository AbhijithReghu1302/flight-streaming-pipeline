{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":66197,"status":"ok","timestamp":1763702641168,"user":{"displayName":"kushagra bhati","userId":"09436809365557822373"},"user_tz":-330},"id":"rHAfpB2wvE22","outputId":"1271dd16-3e77-48c3-979a-3f8ea580f36e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n","Requirement already satisfied: openap in /usr/local/lib/python3.12/dist-packages (2.4)\n","Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.12/dist-packages (from openap) (3.10.0)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from openap) (2.0.2)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from openap) (2.2.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from openap) (6.0.3)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from openap) (1.16.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->openap) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->openap) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->openap) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->openap) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->openap) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->openap) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->openap) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->openap) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->openap) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->openap) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1->openap) (1.17.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n","Requirement already satisfied: pymongo[srv] in /usr/local/lib/python3.12/dist-packages (4.15.4)\n","\u001b[33mWARNING: pymongo 4.15.4 does not provide the extra 'srv'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from pymongo[srv]) (2.8.0)\n","Mounted at /content/drive\n","Spark Session Created.\n","Database Loaded & Broadcasted. Count: 616628\n","Setup Complete.\n"]}],"source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!pip install pyspark\n","!pip install openap\n","!pip install requests\n","!pip install pymongo[srv]\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","from pyspark.sql import SparkSession\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","\n","spark = SparkSession.builder \\\n","    .appName(\"RealTimeAviationPipeline\") \\\n","    .master(\"local[*]\") \\\n","    .config(\"spark.driver.memory\", \"4g\") \\\n","    .getOrCreate()\n","\n","print(\"Spark Session Created.\")\n","\n","csv_path = \"/content/drive/MyDrive/aircraft-database-complete-2025-08.csv\"\n","\n","try:\n","    aircraft_df = spark.read.option(\"header\", \"true\").csv(csv_path)\n","\n","    new_columns = [c.replace(\"'\", \"\") for c in aircraft_df.columns]\n","    aircraft_df = aircraft_df.toDF(*new_columns)\n","\n","    valid_aircraft = aircraft_df.filter(\"typecode IS NOT NULL\").select(\"icao24\", \"typecode\")\n","    aircraft_map = {row['icao24']: row['typecode'] for row in valid_aircraft.collect()}\n","    bc_aircraft_db = spark.sparkContext.broadcast(aircraft_map)\n","\n","    print(f\"Database Loaded & Broadcasted. Count: {len(aircraft_map)}\")\n","\n","except Exception as e:\n","    print(e)\n","\n","os.makedirs(\"/content/flight_data\", exist_ok=True)\n","print(\"Setup Complete.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171,"status":"ok","timestamp":1763703139821,"user":{"displayName":"kushagra bhati","userId":"09436809365557822373"},"user_tz":-330},"id":"OFeS9z565kun","outputId":"8ee372e8-fbb1-4e99-992d-9a88a846246d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Live Streaming Started...\n"]}],"source":["import time\n","import json\n","import requests\n","import threading\n","import os\n","import glob\n","from datetime import datetime\n","\n","stop_streaming = False\n","folder_path = \"/content/flight_data\"\n","\n","def cleanup_old_files():\n","    files = sorted(glob.glob(f\"{folder_path}/*.json\"))\n","\n","    if len(files) > 2:\n","        for f in files[:-2]:\n","            try:\n","                os.remove(f)\n","            except:\n","                pass\n","\n","def fetch_live_data():\n","    batch_id = 0\n","    url = \"https://opensky-network.org/api/states/all\"\n","\n","    while not stop_streaming:\n","        try:\n","            response = requests.get(url, timeout=10)\n","\n","            if response.status_code == 200:\n","                data = response.json()\n","                states = data.get(\"states\", [])\n","\n","                if states:\n","                    timestamp = int(time.time())\n","                    filename = f\"{folder_path}/batch_{timestamp}.json\"\n","\n","                    with open(filename, \"w\") as f:\n","                        json.dump(states, f)\n","\n","                    print(f\"Batch {batch_id} Ingested: {len(states)} flights\")\n","                    batch_id += 1\n","\n","                    cleanup_old_files()\n","            else:\n","                print(f\"API Error: {response.status_code}\")\n","\n","        except Exception as e:\n","            print(f\"Connection Error: {e}\")\n","\n","        time.sleep(100)\n","\n","stream_thread = threading.Thread(target=fetch_live_data)\n","stream_thread.daemon = True\n","stream_thread.start()\n","\n","print(\"Live Streaming Started...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7YrY-3n5zRE"},"outputs":[],"source":["stop_streaming = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":11851,"status":"ok","timestamp":1763703162327,"user":{"displayName":"kushagra bhati","userId":"09436809365557822373"},"user_tz":-330},"id":"N9cwJ_wkGZxt","outputId":"d4ca3bc0-0529-4695-8201-b82886a0b757"},"outputs":[{"name":"stdout","output_type":"stream","text":["Building physics database for 3323 types...\n","Success! Physics Database Broadcasted. Valid Models: 37\n","Verification Passed: B744 MTOW is 396800 kg\n"]}],"source":["from openap import prop\n","\n","\n","unique_types_rows = aircraft_df.select(\"typecode\").distinct().collect()\n","unique_types = [row.typecode for row in unique_types_rows if row.typecode]\n","\n","print(f\"Building physics database for {len(unique_types)} types...\")\n","\n","\n","openap_data = {}\n","\n","for tc in unique_types:\n","    try:\n","\n","        aircraft = prop.aircraft(tc)\n","        if aircraft and 'limits' in aircraft and 'MTOW' in aircraft['limits']:\n","             openap_data[tc] = {\n","                 'mtow': aircraft['limits']['MTOW']\n","             }\n","    except:\n","        pass\n","bc_openap_db = spark.sparkContext.broadcast(openap_data)\n","\n","print(f\"Success! Physics Database Broadcasted. Valid Models: {len(openap_data)}\")\n","if \"B744\" in openap_data:\n","    print(f\"Verification Passed: B744 MTOW is {openap_data['B744']['mtow']} kg\")\n","else:\n","    print(\"Warning: B744 still missing.\")"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1763704977021,"user":{"displayName":"kushagra bhati","userId":"09436809365557822373"},"user_tz":-330},"id":"8DDnG7ht2FTC"},"outputs":[],"source":["import pymongo\n","from datetime import datetime, timezone\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import DoubleType\n","from openap import FuelFlow, Emission\n","\n","MONGO_URI = \"mongodb+srv://<username>:<password>@cluster.mongodb.net/\"\n","DB_NAME = \"AviationIntelligence\"\n","COLLECTION_NAME = \"FlightStream\"\n","\n","DEFAULT_AC_TYPE = \"E190\"\n","DEFAULT_MTOW_KG = 28000.0\n","\n","\n","def lookup_typecode(icao24):\n","    if not icao24: return None\n","    return bc_aircraft_db.value.get(icao24)\n","\n","\n","def determine_phase(altitude, vertical_rate, speed):\n","    if altitude is None or vertical_rate is None or speed is None:\n","        return \"CRUISE\"\n","\n","    if altitude < 200 and vertical_rate < -500:\n","        return \"LANDING\"\n","    elif altitude < 200 and speed < 92.6:\n","        return \"TAXI\"\n","    elif vertical_rate > 500 and altitude < 2000:\n","        return \"TAKEOFF\"\n","    elif vertical_rate > 500:\n","        return \"ASCENDING\"\n","    elif vertical_rate < -500:\n","        return \"DESCENDING\"\n","    else:\n","        return \"CRUISE\"\n","\n","def calculate_fuel_flow(typecode, altitude_ft, speed_kmh, vertical_rate_fpm, phase):\n","    try:\n","        if altitude_ft is None or speed_kmh is None: return None\n","\n","        ac_props = bc_openap_db.value.get(typecode, {})\n","        model_type = typecode if (typecode and ac_props.get('mtow')) else DEFAULT_AC_TYPE\n","        mass_kg = 0.80 * ac_props.get('mtow', DEFAULT_MTOW_KG)\n","\n","        try:\n","            fuelflow = FuelFlow(ac=model_type)\n","        except:\n","            fuelflow = FuelFlow(ac=DEFAULT_AC_TYPE)\n","            mass_kg = DEFAULT_MTOW_KG\n","\n","        alt_m = altitude_ft * 0.3048\n","        speed_ms = speed_kmh / 3.6\n","        vs_ms = (vertical_rate_fpm or 0) * 0.00508\n","\n","        if phase == \"TAXI\":\n","            return fuelflow.at_actuator_model(0, alt=0)\n","        else:\n","            active_vs = vs_ms if phase in [\"ASCENDING\", \"DESCENDING\", \"TAKEOFF\", \"LANDING\"] else 0\n","            return fuelflow.enroute(mass=mass_kg, tas=speed_ms, alt=alt_m, vs=active_vs)\n","    except:\n","        return None\n","\n","def calculate_nox(fuel_flow, typecode, altitude_ft, speed_kmh):\n","    try:\n","        if fuel_flow is None or altitude_ft is None or speed_kmh is None: return None\n","\n","        ac_props = bc_openap_db.value.get(typecode, {})\n","        model_type = typecode if (typecode and ac_props.get('mtow')) else DEFAULT_AC_TYPE\n","\n","        try:\n","            emission = Emission(ac=model_type)\n","        except:\n","            emission = Emission(ac=DEFAULT_AC_TYPE)\n","\n","        alt_m = altitude_ft * 0.3048\n","        speed_ms = speed_kmh / 3.6\n","\n","        return emission.nox(fuel_flow, tas=speed_ms, alt=alt_m)\n","    except:\n","        return None\n","\n","def calculate_co2(fuel_flow):\n","    if fuel_flow is None: return None\n","    return fuel_flow * 3.16\n","\n","def calculate_cost(fuel_flow):\n","    if fuel_flow is None: return None\n","    return fuel_flow * 0.77\n","\n","def save_to_mongodb(spark_df, batch_filename):\n","    try:\n","        data = spark_df.toPandas().to_dict(\"records\")\n","\n","        if not data:\n","            return\n","\n","        ingest_time = datetime.utcnow()\n","\n","        formatted_docs = []\n","        for row in data:\n","            doc = {\n","                \"timestamp\": ingest_time,\n","                \"batch_source\": batch_filename,\n","                \"icao24\": row['icao24'],\n","                \"callsign\": row.get('callsign', '').strip(),\n","                \"country\": row.get('origin_country', '').strip(),\n","                \"position\": {\n","                    \"latitude\": row['latitude'],\n","                    \"longitude\": row['longitude'],\n","                    \"heading\": row['heading']\n","                },\n","                \"telemetry\": {\n","                    \"altitude_ft\": row['geo_altitude_ft'],\n","                    \"speed_kmh\": row['velocity_kmh'],\n","                    \"vertical_rate_fpm\": row['vertical_rate_fpm']\n","                },\n","                \"aircraft\": {\n","                    \"type_code\": row['typecode']\n","                },\n","                \"analytics\": {\n","                    \"phase\": row['phase'],\n","                    \"fuel_rate_kg_s\": row['fuel_rate_kg_s'],\n","                    \"co2_rate_kg_s\": row['co2_rate_kg_s'],\n","                    \"nox_rate_kg_s\": row['nox_rate_kg_s'],\n","                    \"cost_rate_usd_s\": row['cost_rate_usd_s']\n","                }\n","            }\n","            formatted_docs.append(doc)\n","\n","        with pymongo.MongoClient(MONGO_URI) as client:\n","            db = client[DB_NAME]\n","            coll = db[COLLECTION_NAME]\n","            coll.insert_many(formatted_docs)\n","\n","        print(f\"MongoDB: Successfully inserted {len(formatted_docs)} documents.\")\n","\n","    except Exception as e:\n","        print(f\"MongoDB Write Error: {e}\")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"LE2mljVzCkL6","executionInfo":{"status":"ok","timestamp":1763705497989,"user_tz":-330,"elapsed":124898,"user":{"displayName":"kushagra bhati","userId":"09436809365557822373"}},"outputId":"1ee2f178-9902-4dfd-dc7b-c746ba7579cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing Batch: /content/flight_data/batch_1763705274.json\n","Batch 22 Ingested: 5334 flights\n","Batch 23 Ingested: 5362 flights\n","Computed Analytics for 4605 flights.\n","MongoDB Write Error: An error occurred while calling o1139.collectToPython.\n",": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 22) (4283400bb344 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/content/flight_data/batch_1763705274.json does not exist\n","It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n","\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n","\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n","\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n","\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n","\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n","\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.SparkFileNotFoundException: File file:/content/flight_data/batch_1763705274.json does not exist\n","It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n","\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n","\n","Pipeline Error: An error occurred while calling o1152.showString.\n",": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 23) (4283400bb344 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/content/flight_data/batch_1763705274.json does not exist\n","It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n","\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n","\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n","\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n","\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n","\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n","\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n","\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n","\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n","\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n","\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.SparkFileNotFoundException: File file:/content/flight_data/batch_1763705274.json does not exist\n","It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n","\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n","\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n","\n"]}],"source":["from pyspark.sql.functions import col, element_at, from_json, explode, lit, udf\n","from pyspark.sql.types import ArrayType, StringType, DoubleType\n","import glob\n","\n","files = sorted(glob.glob(f\"{folder_path}/*.json\"))\n","\n","if files:\n","    latest_file = files[-1]\n","    print(f\"Processing Batch: {latest_file}\")\n","\n","    try:\n","        text_df = spark.read.text(latest_file)\n","        json_schema = ArrayType(ArrayType(StringType()))\n","        raw_df = text_df.select(explode(from_json(col(\"value\"), json_schema)).alias(\"value\"))\n","\n","        flights_df = raw_df.select(\n","            element_at(col(\"value\"), 1).alias(\"icao24\"),\n","            element_at(col(\"value\"), 2).alias(\"callsign\"),\n","            element_at(col(\"value\"), 3).alias(\"origin_country\"),\n","            element_at(col(\"value\"), 6).cast(\"double\").alias(\"longitude\"),\n","            element_at(col(\"value\"), 7).cast(\"double\").alias(\"latitude\"),\n","            element_at(col(\"value\"), 11).cast(\"double\").alias(\"heading\"),\n","            (element_at(col(\"value\"), 14).cast(\"double\") * 3.28084).alias(\"geo_altitude_ft\"),\n","            (element_at(col(\"value\"), 10).cast(\"double\") * 3.6).alias(\"velocity_kmh\"),\n","            (element_at(col(\"value\"), 12).cast(\"double\") * 196.85).alias(\"vertical_rate_fpm\")\n","        ).filter(\n","            col(\"icao24\").isNotNull() &\n","            col(\"velocity_kmh\").isNotNull() &\n","            col(\"geo_altitude_ft\").isNotNull()\n","        )\n","\n","        lookup_udf = udf(lookup_typecode, StringType())\n","        phase_udf = udf(determine_phase, StringType())\n","        fuel_udf = udf(calculate_fuel_flow, DoubleType())\n","        co2_udf = udf(calculate_co2, DoubleType())\n","        nox_udf = udf(calculate_nox, DoubleType())\n","\n","        flights_df = flights_df.withColumn(\"typecode\", lookup_udf(col(\"icao24\")))\n","\n","        flights_df = flights_df.withColumn(\"phase\",\n","            phase_udf(col(\"geo_altitude_ft\"), col(\"vertical_rate_fpm\"), col(\"velocity_kmh\")))\n","\n","        flights_df = flights_df.withColumn(\"fuel_rate_kg_s\",\n","            fuel_udf(col(\"typecode\"), col(\"geo_altitude_ft\"), col(\"velocity_kmh\"),\n","                     col(\"vertical_rate_fpm\"), col(\"phase\")))\n","\n","        flights_df = flights_df.withColumn(\"co2_rate_kg_s\", co2_udf(col(\"fuel_rate_kg_s\")))\n","        flights_df = flights_df.withColumn(\"nox_rate_kg_s\",\n","            nox_udf(col(\"fuel_rate_kg_s\"), col(\"typecode\"), col(\"geo_altitude_ft\"), col(\"velocity_kmh\")))\n","        flights_df = flights_df.withColumn(\"cost_rate_usd_s\", col(\"fuel_rate_kg_s\") * 0.77)\n","\n","        final_df = flights_df.filter(col(\"fuel_rate_kg_s\").isNotNull())\n","\n","        count = final_df.count()\n","        print(f\"Computed Analytics for {count} flights.\")\n","\n","        if count > 0:\n","            save_to_mongodb(final_df, latest_file)\n","\n","            final_df.select(\"icao24\", \"typecode\", \"fuel_rate_kg_s\", \"co2_rate_kg_s\", \"nox_rate_kg_s\").show(5)\n","\n","    except Exception as e:\n","        print(f\"Pipeline Error: {str(e)}\")\n","\n","else:\n","    print(\"No data files found.\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}